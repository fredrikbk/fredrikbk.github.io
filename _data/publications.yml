cgo25stardust:
  title: "Stardust: Compiling Sparse Tensor Algebra to a Reconfigurable Dataflow Architecture"
  abstract: "<p>
    We introduce Stardust, a compiler from a sparse tensor algebra language to
    a reconfigurable dataflow architecture, by way of the Spatial
    parallel-patterns programming model. The key insight is to let performance
    engineers specify the placement of data into memories separately from the
    placement of computation onto compute units. Data is placed using an
    abstract memory model, and Stardust binds that data to complex, on-chip
    physical memories. Stardust then binds computation that uses on-chip data
    structures to the appropriate parallel patterns. Using cycle-accurate
    simulation, we show that Stardust can generate nine more tensor algebra
    kernels than the original Capstan work. The generated kernels result in
    138x better performance on average than generated CPU kernels and 41x
    better performance on average than generated GPU kernels.
  </p>"
  venue: "International Symposium on Code Generation and Optimization"
  venuenote: "CGO"
  year: 2025
  month: March
  authors:
    - hsu
    - rucker
    - tianzhao
    - varundesai
    - olukotun
    - kjolstad

oopsla24recurrences:
  title: "Compiling Recurrences over Dense and Sparse Arrays"
  abstract: "<p>
    We present a framework for compiling recurrence equations into native code.
    In our framework, users specify a system of recurrences, the types of data
    structures that store inputs and outputs, and scheduling commands for
    optimization. Our compiler then lowers these specifications into native
    code that respects the dependencies in the recurrence equations.  Our
    compiler can generate code over both sparse and dense data structures, and
    determines if the recurrence system is solvable with the provided
    scheduling primitives. We evaluate the performance and correctness of the
    generated code on several recurrences, from domains as diverse as dense and
    sparse matrix solvers, dynamic programming, graph problems, and sparse
    tensor algebra. We demonstrate that the generated code has competitive
    performance to hand-optimized implementations in libraries. However, these
    handwritten libraries target specific recurrences, specific data
    structures, and specific optimizations. Our system, on the other hand,
    automatically generates implementations from recurrences, data formats, and
    schedules, giving our system more generality than library approaches.
    </p>"
  venue: "Proceedings of the ACM on Programming Languages"
  volume: 8
  issue: OOPSLA
  year: 2024
  month: October
  authors:
    - sundram
    - usman
    - kjolstad

isca24dam.md:
  title: "The Dataflow Abstract Machine Simulator Framework"
  abstract: "<p>
    The growing interest in novel dataflow architectures and streaming
    execution paradigms has created the need for a simulator optimized for
    modeling dataflow systems.
    </p>
    <p>
    To fill this need, we present three new techniques that make it feasible to
    simulate complex systems consisting of thousands of components. First, we
    introduce an interface based on Communi- cating Sequential Processes which
    allows users to simultaneously describe functional and timing
    characteristics. Second, we intro- duce a scalable point-to-point
    synchronization scheme that avoids global synchronization. Finally, we
    demonstrate a technique to exploit slack in the simulated system, such as
    FIFOs, to increase simulation parallelism.
    </p>
    <p>
    We implement these techniques in the Dataflow Abstract Machine (DAM), a
    parallel simulator framework for dataflow systems. We demonstrate the
    benefits of using DAM by high- lighting three case studies using the
    framework. First, we use DAM directly as an exploration tool for streaming
    algorithms on dataflow hardware. We simulate two different implementations
    of the attention algorithm used in large language models, and use DAM to
    show that the second implementation only requires a constant amount of
    local memory. Second, we re-implement a simulator for a sparse tensor
    algebra accelerator, resulting in 57% less code and a simulation speedup of
    up to four orders of magnitude. Finally, we demonstrate a general technique
    for time- multiplexing real hardware to simulate multiple virtual copies of
    the hardware using DAM.
    </p>"
  venue: "Proceedings of the International Symposium on Computer Architecture"
  venuenote: "ISCA"
  year: 2024
  month: June
  award: "ISCA Distinguished Artifact Award"
  authors:
    - nathanzhang
    - lacouture
    - ginasohn 
    - paulmure
    - qizhengzhang
    - kjolstad
    - olukotun

pldi24:
  title: "Compilation of Modular and General Sparse Workspaces"
  abstract: "<p>
    Recent years have seen considerable work on compiling sparse tensor algebra
    expressions. This paper addresses a shortcoming in that work, namely how to
    generate efficient code (in time and space) that scatters values into a
    sparse result tensor. We address this shortcoming through a compiler design
    that generates code that uses sparse intermediate tensors (sparse
    workspaces) as efficient adapters between compute code that scatters and
    result tensors that do not support random insertion.  Our compiler
    automatically detects sparse scattering behavior in tensor expressions and
    inserts necessary intermediate workspace tensors. We present an algorithm
    template for workspace insertion that is the backbone of our code
    generation algorithm. Our algorithm template is modular by design,
    supporting sparse workspaces that span multiple user-defined
    implementations.  Our evaluation shows that sparse workspaces can be up to
    27.12x faster than the dense workspaces of prior work. On the other hand,
    dense workspaces can be up to 7.58x faster than the sparse workspaces
    generated by our compiler in other situations, which motivates our compiler
    design that supports both. Our compiler produces sequential code that is
    competitive with hand-optimized linear and tensor algebra libraries on the
    expressions they support, but that generalizes to any other expression.
    Sparse workspaces are also more memory efficient than dense workspaces as
    they compress away zeros. This compression can asymptotically decrease
    memory usage, enabling tensor computations on data that would otherwise run
    out of memory.
  </p>"
  venue: "Proceedings of the ACM on Programming Languages"
  volume: 8
  issue: PLDI 
  year: 2024
  month: June
  pdf: /publications/general-workspaces.pdf
  authors:
    - genghan
    - hsu
    - kjolstad

vlsi24:
  title: "Onyx: A 12nm 756 GOPS/W Coarse-Grained Reconfigurable Array for Accelerating Dense and Sparse Applications"
  abstract: "<p>
    Onyx is the first fully programmable accelerator for arbitrary sparse
    tensor algebra kernels. Unlike prior work, it supports higher-order
    tensors, multiple inputs, and fusion. It achieves this with a
    coarse-grained reconfigurable array (CGRA) that has composable memory
    primitives for storing compressed any-order tensors and compute primitives
    that eliminate ineffectual computations in sparse expressions. Further,
    Onyx improves dense image processing and machine learning (ML) with
    application-specialized compute tiles, memory tiles optimized for affine
    access patterns, and hybrid clock gating in the global buffer. We achieve
    up to 565x better energy-delay product (EDP) for sparse kernels vs. CPUs
    with sparse libraries, and up to 76% and 85% lower EDP for image processing
    and ML, respectively, vs. Amber [1].
  </p>"
  venue: "IEEE Symposium on VLSI Technology and Circuits (VLSI Technology and Circuits)"
  year: 2024
  month: June
  authors:
    - koul
    - strange
    - melchert 
    - carsello
    - mei
    - hsu
    - kong
    - chen2
    - ke
    - zhang
    - liu
    - nyengele
    - balasingam
    - adivarahan
    - sharma
    - xie
    - torng
    - emer
    - kjolstad
    - horowitz
    - raina

hpca24:
  title: "Revet: A Language and Compiler for Dataflow Threads"
  abstract: "<p>
    Spatial dataflow architectures such as reconfigurable dataflow accelerators
    (RDA) can provide much higher performance and efficiency than CPUs and
    GPUs. In particular, vectorized reconfigurable dataflow accelerators (vRDA)
    in recent literature represent a design point that enhances the efficiency
    of dataflow architectures with vectorization. Today, vRDAs can be exploited
    using either hardcoded kernels or MapReduce languages like Spatial, which
    cannot vectorize data-dependent control flow. In contrast, CPUs and GPUs
    can be programmed using general-purpose threaded abstractions.  The ideal
    combination would be the generality of a threaded programming model coupled
    with the efficient execution model of a vRDA. We introduce Revet: a programming
    model, compiler, and execution model that lets threaded applications run
    efficiently on vRDAs. The Revet programming language uses threads to support
    a broader range of applications than Spatial's parallel patterns, and our
    MLIR-based compiler lowers this language to a generic dataflow backend that
    operates on streaming tensors. Finally, we show that mapping threads to
    dataflow outperforms GPUs, the current state-of-the-art for threaded
    accelerators, by 3.8x.
    </p>"
  venue: "IEEE International Symposium on High-Performance Computer Architecture"
  venuenote: "HPCA"
  year: 2024
  month: March
  pdf: /publications/revet.pdf
  authors:
    - rucker
    - sundram
    - csmith
    - vilim
    - prabhakar
    - kjolstad
    - olukotun

sc23-legate-sparse:
  title: "Legate Sparse: Distributed Sparse Computing in Python"
  abstract: "<p>
    The sparse module of the popular SciPy Python library is widely used across
    applications in scientific computing, data analysis, and machine learning.
    The standard implementation of SciPy is restricted to a single CPU and
    cannot take advantage of modern distributed and accelerated computing
    resources. We introduce Legate Sparse, a system that transparently
    distributes and accelerates unmodified sparse matrix-based SciPy programs
    across clusters of CPUs and GPUs, and composes with cuNumeric, a
    distributed NumPy library. Legate Sparse uses a combination of static and
    dynamic techniques to performantly compose independently written sparse and
    dense array programming libraries, providing a unified Python interface for
    distributed sparse and dense array computations. We show that Legate Sparse
    is competitive with single-GPU libraries like CuPy and the
    industry-standard PETSc library on up to 1280 CPU cores and 192 GPUs of the
    Summit supercomputer, while offering the productivity benefits of idiomatic
    SciPy and NumPy.
  </p>"
  venue: "ACM/IEEE International Conference for High Performance Computing, Networking, Storage and Analysis"
  venuenote: "SC"
  year: 2023
  month: November
  pdf: /publications/legate-sparse.pdf
  authors:
    - yadav
    - lee
    - elibol
    - papadakis
    - leepatti
    - garland
    - aiken
    - kjolstad
    - bauer

pldi23-etch:
  title: "Indexed Streams: A Formal Intermediate Representation for the Fused Execution of Contraction Operations"
  abstract: "<p>
    We introduce indexed streams, a formal operational model and intermediate
    representation that describes the fused execution of a contraction language
    that encompasses both sparse tensor algebra and relational algebra. We
    prove that the indexed stream model is correct with respect to a functional
    semantics. We also develop a compiler for contraction expressions that uses
    indexed streams as an intermediate representation. The compiler is only 540
    lines of code, but we show that its performance can match both the TACO
    compiler for sparse tensor algebra and the SQLite and DuckDB query
    processing libraries for relational algebra.
  </p>"
  venue: "Proceedings of the ACM on Programming Languages"
  volume: 7
  issue: PLDI
  year: 2023
  month: June
  pdf: /publications/indexed-streams.pdf
  movie: https://youtu.be/BSis3h_A51Y?si=owxeFuHEIeETsLQk&t=2270
  embeddedmovie: <iframe width="560" height="315" src="https://www.youtube.com/embed/BSis3h_A51Y?si=owxeFuHEIeETsLQk&amp;start=2270" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe> 
  authors:
    - kovach
    - kolichala
    - gu
    - kjolstad

pldi23-mosaic:
  title: "Mosaic: An Interoperable Compiler for Tensor Algebra"
  abstract: "<p>
    We introduce Mosaic, a sparse tensor algebra compiler that binds tensor
    (sub-)expressions to external functions of other tensor algebra libraries
    and compilers. Users can extend Mosaic by adding new functions and can bind
    a sub-computation to a function using a scheduling API. Mosaic substitutes
    the bound (sub-)expressions with the specified function call and
    automatically fills in the rest of the unbound code using a default code
    generator. Mosaic also offers a search system that can automatically map an
    expression to a set of registered external functions. Both the explicit
    binding and automatic search are verified by Mosaic. We demonstrate the
    benefits of our approach by showing that calling hand-written CPU and
    specialized hardware functions can provide speedup of up to 206x (for an
    SDDMM expression) and 173x (for an SpMV expression), respectively, over a
    homogeneous compiler. Mosaic's external function interface is simple and
    general. Currently, 38 external functions have been added to Mosaic, with
    each addition averaging 20 lines of code.
  </p>"
  venue: "Proceedings of the ACM on Programming Languages"
  volume: 7
  issue: PLDI
  year: 2023
  month: June
  award: "Distinguished Paper Award"
  pdf: /publications/mosaic.pdf
  authors:
    - bansal
    - hsu
    - olukotun
    - kjolstad

asplos2023b:
  title: "BaCO: A Fast and Portable Bayesian Compiler Optimization Framework"
  abstract: "<p>
    We introduce the Bayesian Compiler Optimization framework (BaCO), a general
    purpose autotuner for modern compilers targeting CPUs, GPUs, and FPGAs. BaCO
    provides the flexibility needed to handle the requirements of modern autotuning
    tasks. Particularly, it deals with permutation, ordered, and continuous
    parameter types along with both known and unknown parameter constraints. To
    reason about these parameter types and efficiently deliver high-quality code,
    BaCO uses Bayesian optimization algorithms specialized towards the autotuning
    domain. We demonstrate BaCO's effectiveness on three modern compiler systems:
    TACO, RISE & ELEVATE, and HPVM2FPGA for CPUs, GPUs, and FPGAs respectively. For
    these domains, BaCO outperforms current state-of-the-art autotuners by
    delivering on average 1.39x-1.89x faster code with a tiny search budget, and
    BaCO is able to reach expert-level performance 2.89x-8.77x faster.
    </p>"
  venue: "International Conference on Architectural Support for Programming Languages and Operating Systems"
  venuenote: "ASPLOS"
  year: 2023
  month: April
  authors:
    - hellsten
    - souza
    - lenfers
    - lacouture
    - hsu
    - ejjeh
    - kjolstad
    - steuwer
    - olukotun
    - nardi

asplos23:
  title: "The Sparse Abstract Machine"
  abstract: "<p>
    We propose the Sparse Abstract Machine (SAM), an intermediate
    representation and abstract machine model for targeting sparse tensor
    algebra to reconfigurable and fixed-function spatial dataflow accelerators.
    SAM defines a streaming abstraction with sparse primitives that encompass a
    large space of scheduled tensor algebra expressions. SAM dataflow graphs
    naturally separate tensor formats from algorithms and is expressive enough
    to incorporate many sparse-iteration and hardware-specific optimizations.
    We show an automatic compilation technique from a high-level language to
    SAM and a set of hardware primitives which implement it. We evaluate the
    generality and extensibility of our sparse abstract machine, explore the
    performance space of sparse tensor algebra optimizations using SAM, and
    provide an example implementation of our SAM architecture.
  </p>"
  venue: "Architectural Support for Programming Languages and Operating Systems"
  venuenote: "ASPLOS"
  year: 2023
  month: March
  pdf: /publications/sam.pdf
  movie: https://youtu.be/zCzZKWheizE?si=6jwUl0oVAT6ETJ6N&t=18
  embeddedmovie: <iframe width="560" height="315" src="https://www.youtube.com/embed/zCzZKWheizE?si=6jwUl0oVAT6ETJ6N&amp;start=18" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
  authors:
    - hsu
    - strange
    - sharma 
    - won
    - olukotun
    - emer
    - horowitz
    - kjolstad

taco23-ubuffer:
  title: "Unified Buffer: Compiling Image Processing and Machine Learning Applications to Push-Memory Accelerators"
  abstract: "<p>
    Image processing and machine learning applications benefit
    tremendously from hardware acceleration. Existing compilers
    target either FPGAs, which sacrifice power and performance for
    programmability, or ASICs, which become obsolete as
    applications change. Programmable domain-specific
    accelerators, such as coarse-grained reconfigurable arrays
    (CGRAs), have emerged as a promising middle-ground, but they
    have traditionally been difficult compiler targets since they
    use a different memory abstraction. In contrast to CPUs and
    GPUs, the memory hierarchies of domain-specific accelerators
    use push memories: memories that send input data streams to
    computation kernels or to higher or lower levels in the memory
    hierarchy, and store the resulting output data streams. To address
    the compilation challenge caused by push memories, we propose that
    the representation of these memories in the compiler be altered to
    directly represent them by combining storage with address
    generation and control logic in a single structure—a unified
    buffer.
    </p><p>
    The unified buffer abstraction enables the compiler to separate
    generic push memory optimizations from the mapping to specific
    memory implementations in the backend. This separation allows our
    compiler to map high-level Halide applications to different CGRA
    memory designs, including some with a ready-valid interface. The
    separation also opens the opportunity for optimizing push memory
    elements on reconfigurable arrays. Our optimized memory
    implementation, the Physical Unified Buffer (PUB), uses a
    wide-fetch, single-port SRAM macro with built-in address
    generation logic to implement a buffer with two read and two write
    ports. It is 18% smaller and consumes 31% less energy than a
    physical buffer implementation using a dual-port memory that only
    supports two ports.
    </p><p>
    Finally, our system evaluation shows that enabling a compiler to
    support CGRAs leads to performance and energy benefits. Over a
    wide range of image processing and machine learning applications,
    our CGRA achieves 4.7x better runtime and 3.5: better
    energy-efficiency compared to an FPGA.
  </p>"
  venue: "ACM Transactions on Architecture and Code Optimization"
  venuenote: "TACO"
  year: 2023
  month: March 
  pdf: /publications/unified-buffer.pdf
  authors:
    - liu
    - setter 
    - huff
    - strange
    - feng
    - horowitz
    - raina
    - kjolstad

cgo23:
  title: "Looplets: A Language For Structured Coiteration"
  abstract: "<p>
    Real world arrays often contain underlying structure, such as sparsity,
    runs of repeated values, or symmetry. Specializing for structure yields
    significant speedups. But automatically generating efficient code for
    structured data is challenging, especially when arrays with different
    structure interact. We show how to abstract over array structures so that
    the compiler can generate code to coiterate over any combination of them.
    Our technique enables new array formats (such as 1DVBL for irregular
    clustered sparsity), new iteration strategies (such as galloping
    intersections), and new operations over structured data (such as
    concatenation or convolution).
  </p>"
  venue: "International Symposium on Code Generation and Optimization"
  venuenote: "CGO"
  year: 2023
  month: February
  pdf: /publications/looplets.pdf
  authors:
    - willow
    - donenfeld
    - kjolstad
    - amarasinghe

aha23:
  title: "AHA: An agile approach to the design of coarse-grained reconfigurable accelerators and compilers"
  abstract: "<p>
    With the slowing of Moore’s law, computer architects have turned to
    domain-speciic hardware specialization to continue improving the
    performance and eiciency of computing systems. However, specialization
    typically entails signiicant modii- cations to the software stack to
    properly leverage the updated hardware. The lack of a structured approach
    for updating both the compiler and the accelerator in tandem has impeded
    many attempts to systematize this procedure. We propose a new approach to
    enable lexible and evolvable domain-speciic hardware specialization based
    on coarse-grained reconigurable arrays (CGRAs). Our agile methodology
    employs a combination of new programming languages and formal methods to
    automatically generate the accelerator hardware and its compiler from a
    single source of truth. This enables the creation of design-space
    exploration frameworks that automatically generate accelerator
    architectures that approach the eiciencies of hand-designed accelerators,
    with a signiicantly lower design efort for both hardware and compiler
    generation. Our current system accelerates dense linear algebra
    applications, but is modular and can be extended to support other domains.
    Our methodology has the potential to signiicantly improve the productivity
    of hardware-software engineering teams and enable quicker customization and
    deployment of complex accelerator-rich computing systems.
  </p>"
  venue: "ACM Transactions on Embedded Computing Systems"
  venuenote: "TECS"
  year: 2023
  month: January
  pdf: /publications/aha-tecs.pdf
  authors:
    - koul
    - melchert
    - sreedhar
    - truong
    - nyengele
    - zhang
    - liu
    - setter
    - chen2
    - mei 
    - strange
    - daly
    - donovick
    - carsello
    - kong
    - feng
    - huff
    - nayak 
    - setaluri
    - thomas
    - bhagdikar
    - durst
    - myers
    - tsiskaridze
    - richardson
    - bahr
    - fatahalian
    - hanrahan
    - barrett
    - horowitz
    - torng
    - kjolstad
    - raina

sc22-spdistal:
  title: "SpDISTAL: Compiling Distributed Sparse Tensor Computations"
  abstract: "<p>
    We introduce SpDISTAL, a compiler for sparse tensor algebra that
    targets distributed systems. SpDISTAL combines separate descriptions of
    tensor algebra expressions, sparse data structures, data distribution, and
    computation distribution. Thus, it enables distributed execution of sparse
    tensor algebra expressions with a wide variety of sparse data structures
    and data distributions. SpDISTAL is implemented as a C++ library that
    targets a distributed task-based runtime system and can generate code for
    nodes with both multi-core CPUs and multiple GPUs. SpDISTAL generates
    distributed code that achieves performance competitive with hand-written
    distributed functions for specific sparse tensor algebra expressions and
    that outperforms general interpretation-based systems by one to two orders
    of magnitude.
  </p>"
  venue: "ACM/IEEE International Conference for High Performance Computing, Networking, Storage and Analysis"
  venuenote: "SC"
  year: 2022
  month: November
  pdf: /publications/spdistal.pdf
  authors:
    - yadav
    - aiken
    - kjolstad

taco22-mlir-sparse:
  title: "Compiler Support for Sparse Tensor Computations in MLIR"
  abstract: "<p>
    Sparse tensors arise in problems in science, engineering, machine learning,
    and data analytics. Programs that operate on such tensors can exploit
    sparsity to reduce storage requirements and computational time. Developing
    and maintaining sparse software by hand, however, is a complex and
    error-prone task. Therefore, we propose treating sparsity as a property of
    tensors, not a tedious implementation task, and letting a sparse compiler
    generate sparse code automatically from a sparsity-agnostic definition of
    the computation. This paper discusses integrating this idea into MLIR.
  </p>"
  venue: "ACM Transactions on Architecture and Code Optimization"
  venuenote: "TACO"
  year: 2022
  month: September
  pdf: /publications/mlir-sparse.pdf
  movie: https://youtu.be/x-nHc3hBxHM?si=aYuF8sBMRtZZ3MAB&t=14
  embeddedmovie: <iframe width="560" height="315" src="https://www.youtube.com/embed/x-nHc3hBxHM?si=aYuF8sBMRtZZ3MAB&amp;start=14" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe> 
  authors:
    - bik
    - koanantakool
    - shpeisman
    - vasilache
    - zheng
    - kjolstad

pldi22-distal:
  title: "DISTAL: The Distributed Tensor Algebra Compiler"
  abstract: "<p>
    We introduce DISTAL, a compiler for dense tensor algebra that targets
    modern distributed and heterogeneous systems. DISTAL lets users
    independently describe how tensors and computation map onto target machines
    through separate format and scheduling languages. The combination of
    choices for data and computation distribution creates a large design space
    that includes many algorithms from both the past (e.g., Cannon's algorithm)
    and present (e.g., COSMA). DISTAL compiles a tensor algebra domain specific
    language to a distributed task-based runtime system and supports both nodes
    with multi-core CPUs and multiple GPUs. Code generated by DISTAL is
    competitive with optimized codes for matrix multiply on 256 nodes of the
    Lassen supercomputer and outperforms existing systems by between 1.8x to
    3.7x (with a 45.7x outlier) on higher order tensor operations.
  </p>"
  venue: "ACM SIGPLAN Conference on Programming Language Design and Implementation"
  venuenote: "PLDI"
  year: 2022
  month: June
  authors:
    - yadav
    - aiken
    - kjolstad
  pdf: /publications/distal.pdf
  movie: https://youtu.be/1dYngihnfFU?si=ZqhraHBAgJ573G6v 
  embeddedmovie: <iframe width="560" height="315" src="https://www.youtube.com/embed/1dYngihnfFU?si=ZqhraHBAgJ573G6v" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

pldi22-autoscheduling:
  title: Autoscheduling for Sparse Tensor Algebra with an Asymptotic Cost Model
  abstract: "<p>
    While loop reordering and fusion can make big impacts on the
    constant-factor performance of dense tensor programs, the effects on sparse
    tensor programs are asymptotic, often leading to orders of magnitude
    performance differences in practice. Sparse tensors also introduce a choice
    of compressed storage formats that can have asymptotic effects. Research
    into sparse tensor compilers has led to simplified languages that express
    these tradeoffs, but the user is expected to provide a schedule that makes
    the decisions. This is challenging because schedulers must anticipate the
    interaction between sparse formats, loop structure, potential sparsity
    patterns, and the compiler itself. Automating this decision making process
    stands to finally make sparse tensor compilers accessible to end users.
    </p><p>
    We present, to the best of our knowledge, the first automatic asymptotic
    scheduler for sparse tensor programs. We provide an approach to abstractly
    represent the asymptotic cost of schedules and to choose between them. We
    narrow down the search space to a manageably small Pareto frontier of
    asymptotically undominated kernels. We test our approach by compiling these
    kernels with the TACO sparse tensor compiler and comparing them with those
    generated with the default TACO schedules. Our results show that our
    approach reduces the scheduling space by orders of magnitude and that the
    generated kernels perform asymptotically better than those generated using
    the default schedules.
  </p>"
  venue: "ACM SIGPLAN Conference on Programming Language Design and Implementation"
  venuenote: "PLDI"
  year: 2022
  month: June
  authors:
    - ahrens
    - kjolstad
    - amarasinghe
  pdf: /publications/asymptotic-autoscheduling.pdf

copy-and-patch:
  title: Copy-and-Patch Compilation
  abstract: "<p>
    Fast compilation is important when compilation occurs at runtime, such as
    query compilers in modern database systems and WebAssembly virtual machines
    in modern browsers. We present copy-and-patch, an extremely fast
    compilation technique that also produces good quality code. It is capable
    of lowering both high-level languages and low-level bytecode programs to
    binary code, by stitching together code from a large library of binary
    implementation variants. We call these binary implementations stencils
    because they have holes where missing values must be inserted during code
    generation. We show how to construct a stencil library and describe the
    copy-and-patch algorithm that generates optimized binary code.
    </p><p>
    We demonstrate two use cases of copy-and-patch: a compiler for a high-level
    C-like language intended for metaprogramming and a compiler for WebAssembly.
    Our high-level language compiler has negligible compilation cost: it produces
    code from an AST in less time than it takes to construct the AST. We have
    implemented an SQL database query compiler on top of this metaprogramming
    system and show that on TPC-H database benchmarks, copy-and-patch generates
    code two orders of magnitude faster than LLVM -O0 and three orders of
    magnitude faster than higher optimization levels. The generated code runs an
    order of magnitude faster than interpretation and 14% faster than LLVM -O0.
    Our WebAssembly compiler generates code 4.9X-6.5X faster than Liftoff, the
    WebAssembly baseline compiler in Google Chrome. The generated code also
    outperforms Liftoff's by 39%-63% on the Coremark and PolyBenchC WebAssembly
    benchmarks.
    </p>"
  venue: "Proceedings of the ACM on Programming Languages"
  volume: 5
  issue: OOPSLA
  year: 2021
  month: November
  award: "Distinguished Paper Award"
  authors:
    - xu
    - kjolstad
  appendix: /publications/copy-and-patch-appendix.pdf
  pdf: /publications/copy-and-patch.pdf
  movie: https://youtu.be/PaQJcBdwG9Y
  embeddedmovie: <iframe width="560" height="315" src="https://www.youtube.com/embed/PaQJcBdwG9Y" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

oopsla21array:
  title: Compilation of Sparse Array Programming Models 
  abstract: "<p>
    This paper shows how to compile sparse array programming languages. A sparse array 
    programming language is an array programming language that supports element-wise 
    application, reduction, and broadcasting of arbitrary functions over dense and sparse 
    arrays with any fill value. Such a language has great expressive power and can express 
    sparse and dense linear and tensor algebra, functions over images, exclusion and 
    inclusion filters, and even graph algorithms.
    </p><p>
    Our compiler strategy generalizes prior work in the literature on sparse tensor 
    algebra compilation to support any function applied to sparse arrays, instead of 
    only addition and multiplication. To achieve this, we generalize the notion of sparse 
    iteration spaces beyond intersections and unions. These iteration spaces are 
    automatically derived by considering how algebraic properties annotated onto functions 
    interact with the fill values of the arrays. 
    </p><p>
    We then show how to compile these iteration spaces to efficient code. When compared 
    with two widely-used Python sparse array packages, our evaluation shows that we 
    generate built-in sparse array library features with a performance of 1.4× to 
    53.7× when measured against PyData/Sparse for user-defined functions and between 
    0.98× and 5.53× when measured against SciPy/Sparse for sparse array slicing. Our 
    technique outperforms PyData/Sparse by 6.58× to 70.3×, and (where applicable) 
    performs between 0.96× and 28.9× that of a dense NumPy implementation, on end-to-end 
    sparse array applications. We also implement graph linear algebra kernels in 
    our system with a performance of between 0.56× and 3.50× compared to that of 
    the hand-optimized SuiteSparse:GraphBLAS library
    </p>"
  year: 2021
  authors:
    - henry
    - hsu
    - yadav
    - chou
    - olukotun
    - amarasinghe
    - kjolstad
  venue: "Proceedings of the ACM on Programming Languages"
  volume: 5
  issue: OOPSLA
  year: 2021
  month: November
  appendix: /publications/Sparse_Array_Programming_APPENDIX.pdf
  pdf: /publications/Sparse_Array_Programming.pdf
  movie: https://youtu.be/sY_jEfaP8f4 
  embeddedmovie: <iframe width="560" height="315" src="https://www.youtube.com/embed/sY_jEfaP8f4" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe> 
    
oopsla20:
  title: A Sparse Iteration Space Transformation Framework for Sparse Tensor Algebra
  abstract: "<p>
    We address the problem of optimizing sparse tensor algebra in a compiler
    and show how to define standard loop transformations&mdash;split, collapse, and
    reorder&mdash;on sparse iteration spaces.  The key idea is to track the
    transformation functions that map the original iteration space to derived
    iteration spaces. These functions are needed by the code generator to emit
    code that maps coordinates between iteration spaces at runtime, since the
    coordinates in the sparse data structures remain in the original iteration
    space.  The result is a new iteration order that still iterates  over only
    the subset of nonzero coordinates defined by the data structures.  We
    further demonstrate that derived iteration spaces can tile both the
    universe of coordinates and the subset of nonzero coordinates: the former
    is analogous to tiling dense iteration spaces, while the latter tiles
    sparse iteration spaces into statically load-balanced blocks of nonzeros.
    Tiling the space of nonzeros lets the generated code efficiently exploit
    heterogeneous compute resources such as threads, vector units, and GPUs.
    </p><p>
    We implement these concepts and an associated scheduling API in the
    open-source TACO system. The scheduling API can be used by performance
    engineers or it can be the target of an automatic scheduling system. We
    outline one heuristic autoscheduling system, but many other systems are
    both possible and enabled by our API. Using the scheduling API, we show how
    to optimize sparse and mixed sparse-dense tensor algebra expressions on
    both CPUs and GPUs. Our results show that the sparse transformations are
    sufficient to generate code with competitive performance to hand-optimized
    implementations from the literature, while generalizing to all of the
    tensor algebra.
    </p>"
  venue: "Proceedings of the ACM on Programming Languages"
  volume: 4
  issue: OOPSLA
  year: 2020
  month: November
  authors:
    - senanayake
    - hong
    - wang
    - wilson
    - chou
    - kamil
    - amarasinghe
    - kjolstad
  pdf: /publications/taco-scheduling.pdf
  movie: https://youtu.be/0wJsGWA5pTU
  embeddedmovie: <iframe width="560" height="315" src="https://www.youtube.com/embed/0wJsGWA5pTU" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
    
aha20:
  title: Creating an Agile Hardware Design Flow
  abstract: "<p>
  Although an agile approach is standard for software design, how to properly
  adapt this method to hardware is still an open question. This work addresses
  this question while building a system on chip (SoC) with specialized
  accelerators. Rather than using a traditional waterfall design flow, which
  starts by studying the application to be accelerated, we begin by
  constructing a complete flow from an application expressed in a high-level
  domain-specific language (DSL), in our case Halide, to a generic
  coarse-grained reconfigurable array (CGRA).  As our understanding of the
  application grows, the CGRA design evolves, and we have developed a suite of
  tools that tune application code, the compiler, and the CGRA to increase the
  efficiency of the resulting implementation. To meet our continued need to
  update parts of the system while maintaining the end-to-end flow, we have
  created DSL-based hardware generators that not only provide the Verilog
  needed for the implementation of the CGRA, but also create the collateral
  that the compiler/mapper/place and route system needs to configure its
  operation. This work provides a systematic approach for desiging and evolving
  high-performance and energy-efficient hardware-software systems for any
  application domain.
  </p>"
  venue: "Design Automation Conference"
  venuenote: "DAC"
  year: 2020
  month: July
  authors:
    - bahr
    - barrett
    - bhagdikar
    - carsello
    - daly
    - donovick
    - durst
    - fatahalian
    - feng
    - hanrahan
    - hofstee
    - horowitz
    - huff
    - kjolstad
    - kong
    - liu
    - mann
    - melchert
    - nayak
    - niemetz
    - nyengele
    - raina
    - richardson
    - setaluri
    - setter
    - sreedhar
    - strange
    - thomas
    - torng
    - truong
    - tsiskaridze
    - zhang
  pdf: /publications/aha.pdf

spaa20:
  title: "Sparse Tensor Transpositions"
  abstract: "<p>
  We present a new algorithm for transposing sparse tensors called Quesadilla.
  The algorithm converts the sparse tensor data structure to a list of
  coordinates and sorts it with a fast multi-pass radix algorithm that exploits
  knowledge of the requested transposition and the tensors input partial
  coordinate ordering to provably minimize the number of parallel partial
  sorting passes. We evaluate both a serial and a parallel implementation of
  Quesadilla on a set of 19 tensors from the FROSTT collection, a set of
  tensors taken from scientific and data analytic applications. We compare
  Quesadilla and a generalization, Top-2-sadilla to several state of the art
  approaches, including the tensor transposition routine used in the SPLATT
  tensor factorization library. In serial tests, Quesadilla was the best
  strategy for 60% of all tensor and transposition combinations and improved
  over SPLATT by at least 19% in half of the combinations. In parallel tests,
  at least one of Quesadilla or Top-2-sadilla was the best strategy for 52% of
  all tensor and transposition combinations.
  </p>"
  venue: "ACM Symposium on Parallelism in Algorithms and Architectures"
  venuenote: "SPAA brief announcement"
  year: 2020
  month: July
  authors:
    - mueller
    - ahrens
    - chou
    - kjolstad
    - amarasinghe
  tag: transposition
  pdf: /publications/taco-transposition.pdf


pldi20:
  title: "Automatic Generation of Efficient Sparse Tensor Format Conversion Routines"
  abstract: "<p>
  This paper shows how to generate code that efficiently converts sparse
  tensors between disparate storage formats (data layouts) like CSR, DIA, ELL,
  and many others. We decompose sparse tensor conversion into three logical
  phases: coordinate remapping, analysis, and assembly.  We then develop a
  language that precisely describes how different formats group together and
  order a tensor's nonzeros in memory. This enables a compiler to emit code
  that performs complex reorderings (remappings) of nonzeros when converting
  between formats.  We additionally develop a query language that can extract
  complex statistics about sparse tensors, and we show how to emit efficient
  analysis code that computes such queries. Finally, we define an abstract
  interface that captures how data structures for storing a tensor can be
  efficiently assembled given specific statistics about the tensor.  Disparate
  formats can implement this common interface, thus letting a compiler emit
  optimized sparse tensor conversion code for arbitrary combinations of a wide
  range of formats without hard-coding for any specific one.
  </p><p>
  Our evaluation shows that our technique generates sparse tensor conversion
  routines with performance between 0.99 and 2.2x that of hand-optimized
  implementations in two widely used sparse linear algebra libraries, SPARSKIT
  and Intel MKL.  By emitting code that avoids materializing temporaries, our
  technique also outperforms both libraries by between 1.4 and 3.4x for CSC/COO
  to DIA/ELL conversion.
  </p>"
  venue: "ACM SIGPLAN Conference on Programming Language Design and Implementation"
  venuenote: "PLDI"
  year: 2020
  month: June
  authors:
    - chou
    - kjolstad
    - amarasinghe
  tag: conversion
  pdf: /publications/taco-conversion.pdf
  movie: https://youtu.be/s6JRY8qcjvA
  embeddedmovie: <iframe width="560" height="315" src="https://www.youtube.com/embed/s6JRY8qcjvA" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

cgo19:
  title: "Tensor Algebra Compilation with Workspaces"
  abstract: "<p>
  This paper shows how to extend sparse tensor algebra compilers to
  introduce temporary tensors called workspaces to avoid inefficient sparse
  data structures accesses.  We develop an intermediate representation (IR) for
  tensor operations called concrete index notation that specifies when
  sub-computations occur and where they are stored.  We then describe the
  workspace transformation in this IR, how to programmatically invoke it, and
  how the IR is compiled to sparse code.  Finally, we show how the
  transformation can be used to optimize sparse tensor kernels, including
  sparse matrix multiplication, sparse tensor addition, and the matricized
  tensor times Khatri-Rao product (MTTKRP).
  </p><p>
  Our results show that the workspace transformation brings the performance of
  these kernels on par with hand-optimized implementations. For example, we
  improve the performance of MTTKRP with dense output by up to 35%, and enable
  generating sparse matrix multiplication and MTTKRP with sparse output,
  neither of which were supported by prior tensor algebra compilers.
  </p>"
  venue: "International Symposium on Code Generation and Optimization"
  venuenote: "CGO"
  year: 2019
  month: February
  authors:
    - kjolstad
    - ahrens
    - kamil
    - amarasinghe
  tag: workspaces
  pdf: /publications/taco-workspaces.pdf

oopsla18:
  title: "Format Abstraction for Sparse Tensor Algebra Compilers"
  abstract: "This paper shows how to build a sparse tensor algebra
  compiler that is agnostic to tensor formats (data layouts). We
  develop an interface that describes formats in terms of their
  capabilities and properties, and show how to build a modular code
  generator where new formats can be added as plugins. We then
  describe six implementations of the interface that compose to form
  the dense, CSR/CSF, COO, DIA, ELL, and HASH tensor formats and
  countless variants thereof. With these implementations at hand, our
  code generator can generate code to compute any tensor algebra
  expression on any combination of the aforementioned formats.  To
  demonstrate our technique, we have implemented it in the taco tensor
  algebra compiler. Our modular code generator design makes it simple
  to add support for new tensor formats, and the performance of the
  generated code is competitive with hand-optimized implementations.
  Furthermore, by extending taco to support a wider range of formats
  specialized for different application and data characteristics, we
  can improve end-user application performance. For example, if input
  data is provided in the COO format, our technique allows computing a
  single matrix-vector multiplication directly with the data in COO,
  which is up to 3.6x faster than by first converting the data to
  CSR."
  venue: "Proceedings of the ACM on Programming Languages"
  volume: 2
  issue: OOPSLA
  year: 2018
  month: October
  authors:
    - chou
    - kjolstad
    - amarasinghe
  tag: formats
  pdf: /publications/taco-formats.pdf
  movie: https://youtu.be/sQOq3Ci4tB0
  embeddedmovie: <iframe width="560" height="315" src="https://www.youtube.com/embed/sQOq3Ci4tB0" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

avancees18:
  title: "Taco: compilation et génération de code d’expressions tensorielles"
  abstract: "L’évaluation des performances du code généré démontre l’efficacité
  de cette approche tant sur des expressions simples comme un SpMV de forme bi=
  Aij× xj (figure 3a) que sur des expressions complexes impliquant des tenseurs
  d’ordre élevé comme un MTTKRP de forme Aij= Bikl× Ckj× Dlj. Elle permet
  surtout d’éliminer le compromis entre performance et exhaustivité des
  librairies existantes. La figure 3b montre pour l’expression Aij= Bij+ Cij+
  Dij les gains de performance obtenus avec taco contre les librairies qui
  implémentent cette expression. Le parallélisme pour des architectures à
  mémoire partagée est assuré par l’ajout de directives d’interfaces de
  programmation parallèle (telles que OpenMP ou Cilk) au niveau des boucles
  externes. Les travaux de recherche en cours portent sur la prise en compte de
  nouveaux formats de stockage et d’un niveau de parallélisme distribué. La
  suite logicielle taco est composée de trois couches logicielles successives
  et utilisables indépendamment: une librairie C++, un outil en ligne de
  commande et une interface web. L’outil en ligne de commande permet de
  comparer les performances des noyaux de code générés, d’étudier l’impact du
  choix des formats et d’optimiser interactivement le code généré 4. Le
  compilateur taco propose une approche générative dédiée à l’algèbre
  tensorielle et pour une infinité de formats de stockage. En s’ appuyant sur
  ce compilateur, on peut construire des langages, des bibliothèques et des
  logiciels de simulation manipulant des tenseurs denses et creux sans écrire
  manuellement le code ni sacrifier les performances."
  authors:
    - lugato
    - kjolstad
    - chou
    - amarasinghe
    - kamil
  venue: "AVANCÉES"
  volume: 12
  year: 2018
  tag: avancee
  pdf: https://scholar.google.com/scholar?oi=bibs&cluster=8503015401540923787&btnI=1&hl=en

ase17:
  title: "taco: A Tool to Generate Tensor Algebra Kernels"
  abstract: "Tensor algebra is an important computational abstraction
  that is increasingly used in data analytics, machine learning,
  engineering, and the physical sciences. However, the number of
  tensor expressions is unbounded, which makes it hard to develop and
  optimize libraries. Furthermore, the tensors are often sparse (most
  components are zero), which means the code has to traverse
  compressed formats. To support programmers we have developed taco, a
  code generation tool that generates dense, sparse, and mixed kernels
  from tensor algebra expressions. This paper describes the taco web
  and command-line tools and discusses the benefits of a code
  generator over a traditional library. See also the demo video at
  tensor-compiler.org/ase2017."
  venue: "32th IEEE/ACM International Conference on Automated Software Engineering"
  venuenote: "ASE tools paper"
  year: "2017"
  month: "November"
  authors:
    - kjolstad
    - chou
    - lugato
    - kamil
    - amarasinghe
  tag: tools
  pdf: /publications/taco-tools.pdf
  movie: https://youtu.be/eE38PC2ctFs
  embeddedmovie: <iframe width="560" height="315" src="https://www.youtube.com/embed/eE38PC2ctFs" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe> 

oopsla17:
  title: "The Tensor Algebra Compiler" 
  abstract: "Tensor algebra is a powerful tool with applications in
  machine learning, data analytics, engineering and the physical
  sciences. Tensors are often sparse and compound operations must
  frequently be computed in a single kernel for performance and to
  save memory. Programmers are left to write kernels for every
  operation of interest, with different mixes of dense and sparse
  tensors in different formats. The combinations are infinite, which
  makes it impossible to manually implement and optimize them all.
  This paper introduces the first compiler technique to automatically
  generate kernels for any compound tensor algebra operation on dense
  and sparse tensors. The technique is implemented in a C++ library
  called taco. Its performance is competitive with best-in-class
  hand-optimized kernels in popular libraries, while supporting far
  more tensor operations." 
  venue: "Proceedings of the ACM on Programming Languages"
  volume: 1
  issue: OOPSLA
  year: 2017
  month: October
  award: "Distinguished Paper Award"
  authors:
    - kjolstad 
    - kamil
    - chou
    - lugato
    - amarasinghe
  tag: taco
  pdf: /publications/taco.pdf
  movie: https://youtu.be/Kffbzf9etLE 
  embeddedmovie: <iframe width="560" height="315" src="https://www.youtube.com/embed/Kffbzf9etLE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>

tog16a:
  title: "Simit: A Language for Physical Simulation"
  abstract: "<p>
  With existing programming tools, writing high-performance simulation code is
  labor intensive and requires sacrificing readability and portability. The
  alternative is to prototype simulations in a high-level language like Matlab,
  thereby sacrificing performance. The Matlab programming model naturally
  describes the behavior of an entire physical system using the language of
  linear algebra. However, simulations also manipulate individual geometric
  elements, which are best represented using linked data structures like
  meshes. Translating between the linked data structures and linear algebra
  comes at significant cost, both to the programmer and to the machine.
  High-performance implementations avoid the cost by rephrasing the computation
  in terms of linked or index data structures, leaving the code complicated and
  monolithic, often increasing its size by an order of magnitude.
  </p><p>
  In this article, we present Simit, a new language for physical simulations
  that lets the programmer view the system both as a linked data structure in
  the form of a hypergraph and as a set of global vectors, matrices, and
  tensors depending on what is convenient at any given time. Simit provides a
  novel assembly construct that makes it conceptually easy and computationally
  efficient to move between the two abstractions. Using the information pro-
  vided by the assembly construct, the compiler generates efficient in-place
  computation on the graph. We demonstrate that Simit is easy to use: a Simit
  program is typically shorter than a Matlab program; that it is high
  performance: a Simit program running sequentially on a CPU performs compara-
  bly to hand-optimized simulations; and that it is portable: Simit programs
  can be compiled for GPUs with no change to the program, delivering 4 to 20x
  speedups over our optimized CPU code.
  </p>"
  year: "2016"
  month: "May"
  venue: "ACM Transactions on Graphics"
  venuenote: "TOG presented at SIGGRAPH 2016"
  authors:
    - kjolstad
    - kamil
    - ragan-kelley
    - levin
    - sueda
    - chen
    - vouga
    - kaufman
    - kanwar
    - matusik
    - amarasinghe
  tag: simit
  pdf: /publications/simit.pdf
  movie: https://youtu.be/raPkxhHy5ro
  image: http://groups.csail.mit.edu/commit/papers/2016/simit.jpg
  embeddedmovie: <iframe width="560" height="315" src="https://www.youtube.com/embed/raPkxhHy5ro" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>

tog16b:
  title: "Why New Programming Languages for Simulation?"
  abstract: "<p>
  Simulations are complicated, performance-critical applications that combine
  sophisticated computer science data structures with advanced mathematical
  computation. Best practice suggests using optimized linear algebra libraries,
  yet programmers of high- performance simulations invariably abandon this
  ideal in order to optimize computation around application data structures.
  Getting good performance requires a lot of human effort to manage data
  layout, vectorize, and parallelize code. And the situation is getting worse.
  High-performance systems in the near future will need to support concurrent
  execution on GPUs, multicore CPUs, a host of new architectures, and across
  distributed systems.
  </p><p>
  In response to similar trends, graphics researchers have proposed new
  programming languages to manage and abstract away from hardware complexity.
  Renderman [Upstill 1989], GPU Shading Languages, Cuda [Nvidia 2008], and more
  recently Halide [Ragan-Kelley et al. 2013] and Darkroom [Hegarty et al. 2014]
  are excellent examples. These languages promise considerable reductions in
  programmer effort, as reflected both by the amount of code that must be
  written and the degree to which programmers must optimize for specific
  hardware.  We believe simulation is now primed to move to such programming
  languages. Although simulation is a complicated domain, we are starting to
  understand how to represent a large class of simulations with simple,
  general, and flexible high level programming language concepts built on solid
  foundations. We believe the ideas behind the relational algebra and modern
  databases have direct relevance to simulation, and we expect to see similar
  benefits to those the database community has accrued.
  </p><p>
  In these proceedings, two such languages are presented: Ebb [Bernstein et al.
  2016] and Simit [Kjolstad et al. 2016]. These languages were developed
  independently by two separate groups centered at Stanford and MIT, and we
  believe they are first steps in a larger roadmap. We will let the articles
  speak for themselves on the strengths of their different operators and data
  structures. However, we encourage readers to note the common foundations we
  will lay out and join us in exploring a fascinating new research direction.
  </p>"
  venue: "ACM Transactions on Graphics"
  venuenote: "TOG perspective"
  year: 2016
  month: May
  authors:
    - bernstein
    - kjolstad
  tag: why
  pdf: /publications/why_simulation_languages.pdf

eurompi13:
  title: "MPI Datatype Processing using Runtime Compilation"
  abstract: "Data packing before and after communication can make up as much as
  90% of the communication time on modern computers. Despite MPI's well-defined
  datatype interface for non-contiguous data access, many codes use manual pack
  loops for performance reasons. Programmers write access-pattern specific pack
  loops (e.g., do manual unrolling) for which compilers emit optimized code. In
  contrast, MPI implementations in use today interpret datatypes at pack time,
  resulting in high overheads. In this work we explore the effectiveness of
  using runtime compilation techniques to generate efficient and optimized pack
  code for MPI datatypes at commit time. Thus, none of the overhead of datatype
  interpretation is incurred at pack time and pack setup is as fast as calling
  a function pointer. We have implemented a library called libpack that can be
  used to compile and (un)pack MPI datatypes. The library optimizes the
  datatype representation and uses the LLVM framework to produce vectorized
  machine code for each datatype at commit time. We show several examples of
  how MPI datatype pack functions benefit from runtime compilation and analyze
  the performance of compiled pack functions for the data access patterns in
  many applications. We show that the pack/unpack functions generated by our
  packing library are seven times faster than those of prevalent MPI
  implementations for 73% of the datatypes used in MILC and in many cases
  outperform manual pack loops."
  venue: "20th European MPI Users' Group Meeting"
  year: "2013"
  month: "September"
  award: "Best Paper Award" # (1/25)
  authors:
    - schneider
    - kjolstad
    - hoefler
  tag: datatypecompilation
  pdf: /publications/datatype_compilation.pdf

ppopp12:
  title: "Automatic Datatype Generation and Optimization"
  abstract: "Many high performance applications spend considerable time packing
  noncontiguous data into contiguous communication buffers. MPI Datatypes
  provide an alternative by describing noncontiguous data layouts. This allows
  sophisticated hardware to retrieve data directly from application data
  structures. However, packing codes in real-world applications are often
  complex and specifying equivalent datatypes is difficult, time-consuming, and
  error prone. We present an algorithm that automates the transformation. We
  have implemented the algorithm in a tool that transforms packing code to MPI
  Datatypes, and evaluated it by transforming 90 packing codes from the NAS
  Parallel Benchmarks. The transformation allows easy porting of applications
  to new machines that benefit from datatypes, thus improving programmer
  productivity."
  venue: "17th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming"
  venuenote: "PPoPP short paper"
  year: "2012"
  month: "February"
  authors:
    - kjolstad
    - hoefler
    - snir
  tag: datatypegeneration
  pdf: /publications/datatype_generation.pdf

icse11:
  title: "Transformation for Class Immutability"
  abstract: "It is common for object-oriented programs to have both mutable and
  immutable classes. Immutable classes simplify programing because the
  programmer does not have to reason about side-effects. Sometimes programmers
  write immutable classes from scratch, other times they transform mutable into
  immutable classes. To transform a mutable class, programmers must find all
  methods that mutate its transitive state and all objects that can enter or
  escape the state of the class. The analyses are non-trivial and the rewriting
  is tedious. Fortunately, this can be automated.
  <br/>
  We present an algorithm and a tool, Immutator, that enables the programmer to
  safely transform a mutable class into an immutable class. Two case studies
  and one controlled experiment show that Immutator is useful. It (i) reduces
  the burden of making classes immutable, (ii) is fast enough to be used
  interactively, and (iii) is much safer than manual transformations."
  venue: "33rd International Conference on Software Engineering"
  venuenote: "ICSE"
  year: "2011"
  month: "May"
  authors:
    - kjolstad
    - dig
    - acevedo
    - snir
  tag: immutator
  pdf: /publications/immutator.pdf

cap10:
  title: "Bringing the HPC Programmer's IDE into the 21st Century through Refactoring"
  abstract: "Programming tools for High Performance Computing are lagging
  behind the tools that have improved the productivity of desktop programmers.
  The increasing complexity of HPC codes, the growing number of cores that they
  must utilize, their long life-span, and the plethora of desirable source code
  optimizations and hardware platforms make HPC codes hard to maintain.
  Refactoring tools can enable HPC programmers to explore the space of
  performance optimizations and parallel constructs safely and efficiently.
  <br/>
  This position paper presents our view on how HPC programming tools should
  evolve, a growing catalog of refactorings for HPC programmers, and reports on
  our initial effort to automate some of these refactorings."
  venue: "SPLASH 2010 Workshop on Concurrency for the Application Programmer"
  year: 2010
  month: "October"
  authors:
    - kjolstad
    - dig
    - snir
  tag: hpcrefactoring
  pdf: /publications/hpc_refactoring.pdf

paraplop10:
  title: "Ghost Cell Pattern"
  abstract: "Many problems consist of a structured grid of points that are
  updated repeatedly based on the values of a fixed set of neighboring points
  in the same grid. To parallelize these problems we can geometrically divide
  the grid into chunks that are processed by different processors. One
  challenge with this approach is that the update of points at the periphery of
  a chunk requires values from neighboring chunks. These are often located in
  remote memory belonging to different processes. The naive implementation
  results in a lot of time spent on communication leaving less time for useful
  computation. By using the Ghost Cell Pattern communication overhead can be
  reduced. This results in faster time to completion."
  venue: "2nd Annual Workshop on Parallel Programming Patterns"
  year: "2010"
  month: "March"
  authors:
    - kjolstad
    - snir
  tag: ghost
  pdf: /publications/ghost_cell_pattern.pdf

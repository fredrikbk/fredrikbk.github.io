aha22:
  title: "Aha: An agile approach to the design of coarse-grained reconfigurable accelerators and compilers"
  abstract: "<p>
    With the slowing of Moore’s law, computer architects have turned to
    domain-speciic hardware specialization to continue improving the
    performance and eiciency of computing systems. However, specialization
    typically entails signiicant modii- cations to the software stack to
    properly leverage the updated hardware. The lack of a structured approach
    for updating both the compiler and the accelerator in tandem has impeded
    many attempts to systematize this procedure. We propose a new approach to
    enable lexible and evolvable domain-speciic hardware specialization based
    on coarse-grained reconigurable arrays (CGRAs). Our agile methodology
    employs a combination of new programming languages and formal methods to
    automatically generate the accelerator hardware and its compiler from a
    single source of truth. This enables the creation of design-space
    exploration frameworks that automatically generate accelerator
    architectures that approach the eiciencies of hand-designed accelerators,
    with a signiicantly lower design efort for both hardware and compiler
    generation. Our current system accelerates dense linear algebra
    applications, but is modular and can be extended to support other domains.
    Our methodology has the potential to signiicantly improve the productivity
    of hardware-software engineering teams and enable quicker customization and
    deployment of complex accelerator-rich computing systems.
  </p>"
  venue: "ACM Transactions on Embedded Computing Systems"
  venuenote: "TECS"
  year: 2022
  month: April
  authors:
    - koul
    - melchert
    - sreedhar
    - truong
    - nyengele
    - zhang
    - liu
    - setter
    - chen2
    - mei 
    - strange
    - daly
    - donovick
    - carsello
    - kong
    - feng
    - huff
    - nayak 
    - setaluri
    - thomas
    - bhagdikar
    - durst
    - myers
    - tsiskaridze
    - richardson
    - bahr
    - fatahalian
    - hanrahan
    - barrett
    - horowitz
    - torng
    - kjolstad
    - raina

looplets:
  title: "Looplets: A Language For Structured Coiteration"
  abstract: "<p>
    Real world arrays often contain underlying structure, such as sparsity,
    runs of repeated values, or symmetry. Specializing for structure yields
    significant speedups. But automatically generating efficient code for
    structured data is challenging, especially when arrays with different
    structure interact. We show how to abstract over array structures so that
    the compiler can generate code to coiterate over any combination of them.
    Our technique enables new array formats (such as 1DVBL for irregular
    clustered sparsity), new iteration strategies (such as galloping
    intersections), and new operations over structured data (such as
    concatenation or convolution).
  </p>"
  venue: "International Symposium on Code Generation and Optimization (accepted)"
  venuenote: "CGO"
  year: 2023
  month: February
  authors:
    - willow
    - donenfeld
    - kjolstad
    - amarasinghe

sc22-spdistal:
  title: "SpDISTAL: Compiling Distributed Sparse Tensor Computations"
  abstract: "<p>
    DescriptionWe introduce SpDISTAL, a compiler for sparse tensor algebra that
    targets distributed systems. SpDISTAL combines separate descriptions of
    tensor algebra expressions, sparse data structures, data distribution, and
    computation distribution. Thus, it enables distributed execution of sparse
    tensor algebra expressions with a wide variety of sparse data structures
    and data distributions. SpDISTAL is implemented as a C++ library that
    targets a distributed task-based runtime system and can generate code for
    nodes with both multi-core CPUs and multiple GPUs. SpDISTAL generates
    distributed code that achieves performance competitive with hand-written
    distributed functions for specific sparse tensor algebra expressions and
    that outperforms general interpretation-based systems by one to two orders
    of magnitude.
  </p>"
  venue: "ACM/IEEE International Conference for High Performance Computing, Networking, Storage and Analysis"
  venuenote: "SC"
  year: 2022
  month: November
  authors:
    - yadav
    - aiken
    - kjolstad

taco22-mlir-sparse:
  title: "Compiler Support for Sparse Tensor Computations in MLIR"
  abstract: "<p>
    Sparse tensors arise in problems in science, engineering, machine learning,
    and data analytics. Programs that operate on such tensors can exploit
    sparsity to reduce storage requirements and computational time. Developing
    and maintaining sparse software by hand, however, is a complex and
    error-prone task. Therefore, we propose treating sparsity as a property of
    tensors, not a tedious implementation task, and letting a sparse compiler
    generate sparse code automatically from a sparsity-agnostic definition of
    the computation. This paper discusses integrating this idea into MLIR.
  </p>"
  venue: "ACM Transactions on Architecture and Code Optimization"
  venuenote: "TACO"
  year: 2022
  month: August
  authors:
    - bik
    - koanantakool
    - shpeisman
    - vasilache
    - zheng
    - kjolstad

pldi22-distal:
  title: "DISTAL: The Distributed Tensor Algebra Compiler"
  abstract: "<p>
    We introduce DISTAL, a compiler for dense tensor algebra that targets
    modern distributed and heterogeneous systems. DISTAL lets users
    independently describe how tensors and computation map onto target machines
    through separate format and scheduling languages. The combination of
    choices for data and computation distribution creates a large design space
    that includes many algorithms from both the past (e.g., Cannon's algorithm)
    and present (e.g., COSMA). DISTAL compiles a tensor algebra domain specific
    language to a distributed task-based runtime system and supports both nodes
    with multi-core CPUs and multiple GPUs. Code generated by DISTAL is
    competitive with optimized codes for matrix multiply on 256 nodes of the
    Lassen supercomputer and outperforms existing systems by between 1.8x to
    3.7x (with a 45.7x outlier) on higher order tensor operations.
  </p>"
  venue: "ACM SIGPLAN Conference on Programming Language Design and Implementation"
  venuenote: "PLDI"
  year: 2022
  month: June
  authors:
    - yadav
    - aiken
    - kjolstad
  pdf: /publications/distal.pdf

pldi22-autoscheduling:
  title: Autoscheduling for Sparse Tensor Algebra with an Asymptotic Cost Model
  abstract: "<p>
    While loop reordering and fusion can make big impacts on the
    constant-factor performance of dense tensor programs, the effects on sparse
    tensor programs are asymptotic, often leading to orders of magnitude
    performance differences in practice. Sparse tensors also introduce a choice
    of compressed storage formats that can have asymptotic effects. Research
    into sparse tensor compilers has led to simplified languages that express
    these tradeoffs, but the user is expected to provide a schedule that makes
    the decisions. This is challenging because schedulers must anticipate the
    interaction between sparse formats, loop structure, potential sparsity
    patterns, and the compiler itself. Automating this decision making process
    stands to finally make sparse tensor compilers accessible to end users.
    </p><p>
    We present, to the best of our knowledge, the first automatic asymptotic
    scheduler for sparse tensor programs. We provide an approach to abstractly
    represent the asymptotic cost of schedules and to choose between them. We
    narrow down the search space to a manageably small Pareto frontier of
    asymptotically undominated kernels. We test our approach by compiling these
    kernels with the TACO sparse tensor compiler and comparing them with those
    generated with the default TACO schedules. Our results show that our
    approach reduces the scheduling space by orders of magnitude and that the
    generated kernels perform asymptotically better than those generated using
    the default schedules.
  </p>"
  venue: "ACM SIGPLAN Conference on Programming Language Design and Implementation"
  venuenote: "PLDI"
  year: 2022
  month: June
  authors:
    - ahrens
    - kjolstad
    - amarasinghe
  pdf: /publications/asymptotic-autoscheduling.pdf

copy-and-patch:
  title: Copy-and-Patch Compilation
  abstract: "<p>
    Fast compilation is important when compilation occurs at runtime, such as
    query compilers in modern database systems and WebAssembly virtual machines
    in modern browsers. We present copy-and-patch, an extremely fast
    compilation technique that also produces good quality code. It is capable
    of lowering both high-level languages and low-level bytecode programs to
    binary code, by stitching together code from a large library of binary
    implementation variants. We call these binary implementations stencils
    because they have holes where missing values must be inserted during code
    generation. We show how to construct a stencil library and describe the
    copy-and-patch algorithm that generates optimized binary code.
    </p><p>
    We demonstrate two use cases of copy-and-patch: a compiler for a high-level
    C-like language intended for metaprogramming and a compiler for WebAssembly.
    Our high-level language compiler has negligible compilation cost: it produces
    code from an AST in less time than it takes to construct the AST. We have
    implemented an SQL database query compiler on top of this metaprogramming
    system and show that on TPC-H database benchmarks, copy-and-patch generates
    code two orders of magnitude faster than LLVM -O0 and three orders of
    magnitude faster than higher optimization levels. The generated code runs an
    order of magnitude faster than interpretation and 14% faster than LLVM -O0.
    Our WebAssembly compiler generates code 4.9X-6.5X faster than Liftoff, the
    WebAssembly baseline compiler in Google Chrome. The generated code also
    outperforms Liftoff's by 39%-63% on the Coremark and PolyBenchC WebAssembly
    benchmarks.
    </p>"
  venue: "Proceedings of the ACM on Programming Languages"
  volume: 5
  issue: OOPSLA
  year: 2021
  month: November
  award: "Distinguished Paper Award"
  authors:
    - xu
    - kjolstad
  appendix: /publications/copy-and-patch-appendix.pdf
  pdf: /publications/copy-and-patch.pdf
  movie: https://youtu.be/PaQJcBdwG9Y
  embeddedmovie: <iframe width="560" height="315" src="https://www.youtube.com/embed/PaQJcBdwG9Y" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

oopsla21array:
  title: Compilation of Sparse Array Programming Models 
  abstract: "<p>
    This paper shows how to compile sparse array programming languages. A sparse array 
    programming language is an array programming language that supports element-wise 
    application, reduction, and broadcasting of arbitrary functions over dense and sparse 
    arrays with any fill value. Such a language has great expressive power and can express 
    sparse and dense linear and tensor algebra, functions over images, exclusion and 
    inclusion filters, and even graph algorithms.
    </p><p>
    Our compiler strategy generalizes prior work in the literature on sparse tensor 
    algebra compilation to support any function applied to sparse arrays, instead of 
    only addition and multiplication. To achieve this, we generalize the notion of sparse 
    iteration spaces beyond intersections and unions. These iteration spaces are 
    automatically derived by considering how algebraic properties annotated onto functions 
    interact with the fill values of the arrays. 
    </p><p>
    We then show how to compile these iteration spaces to efficient code. When compared 
    with two widely-used Python sparse array packages, our evaluation shows that we 
    generate built-in sparse array library features with a performance of 1.4× to 
    53.7× when measured against PyData/Sparse for user-defined functions and between 
    0.98× and 5.53× when measured against SciPy/Sparse for sparse array slicing. Our 
    technique outperforms PyData/Sparse by 6.58× to 70.3×, and (where applicable) 
    performs between 0.96× and 28.9× that of a dense NumPy implementation, on end-to-end 
    sparse array applications. We also implement graph linear algebra kernels in 
    our system with a performance of between 0.56× and 3.50× compared to that of 
    the hand-optimized SuiteSparse:GraphBLAS library
    </p>"
  year: 2021
  authors:
    - henry
    - hsu
    - yadav
    - chou
    - olukotun
    - amarasinghe
    - kjolstad
  venue: "Proceedings of the ACM on Programming Languages"
  volume: 5
  issue: OOPSLA
  year: 2021
  month: November
  appendix: /publications/Sparse_Array_Programming_APPENDIX.pdf
  pdf: /publications/Sparse_Array_Programming.pdf
  movie: https://youtu.be/sY_jEfaP8f4 
  embeddedmovie: <iframe width="560" height="315" src="https://www.youtube.com/embed/sY_jEfaP8f4" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe> 
    
oopsla20:
  title: A Sparse Iteration Space Transformation Framework for Sparse Tensor Algebra
  abstract: "<p>
    We address the problem of optimizing sparse tensor algebra in a compiler
    and show how to define standard loop transformations&mdash;split, collapse, and
    reorder&mdash;on sparse iteration spaces.  The key idea is to track the
    transformation functions that map the original iteration space to derived
    iteration spaces. These functions are needed by the code generator to emit
    code that maps coordinates between iteration spaces at runtime, since the
    coordinates in the sparse data structures remain in the original iteration
    space.  The result is a new iteration order that still iterates  over only
    the subset of nonzero coordinates defined by the data structures.  We
    further demonstrate that derived iteration spaces can tile both the
    universe of coordinates and the subset of nonzero coordinates: the former
    is analogous to tiling dense iteration spaces, while the latter tiles
    sparse iteration spaces into statically load-balanced blocks of nonzeros.
    Tiling the space of nonzeros lets the generated code efficiently exploit
    heterogeneous compute resources such as threads, vector units, and GPUs.
    </p><p>
    We implement these concepts and an associated scheduling API in the
    open-source TACO system. The scheduling API can be used by performance
    engineers or it can be the target of an automatic scheduling system. We
    outline one heuristic autoscheduling system, but many other systems are
    both possible and enabled by our API. Using the scheduling API, we show how
    to optimize sparse and mixed sparse-dense tensor algebra expressions on
    both CPUs and GPUs. Our results show that the sparse transformations are
    sufficient to generate code with competitive performance to hand-optimized
    implementations from the literature, while generalizing to all of the
    tensor algebra.
    </p>"
  venue: "Proceedings of the ACM on Programming Languages"
  volume: 4
  issue: OOPSLA
  year: 2020
  month: November
  authors:
    - senanayake
    - hong
    - wang
    - wilson
    - chou
    - kamil
    - amarasinghe
    - kjolstad
  pdf: /publications/taco-scheduling.pdf
  movie: https://youtu.be/0wJsGWA5pTU
  embeddedmovie: <iframe width="560" height="315" src="https://www.youtube.com/embed/0wJsGWA5pTU" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
    
aha20:
  title: Creating an Agile Hardware Design Flow
  abstract: "<p>
  Although an agile approach is standard for software design, how to properly
  adapt this method to hardware is still an open question. This work addresses
  this question while building a system on chip (SoC) with specialized
  accelerators. Rather than using a traditional waterfall design flow, which
  starts by studying the application to be accelerated, we begin by
  constructing a complete flow from an application expressed in a high-level
  domain-specific language (DSL), in our case Halide, to a generic
  coarse-grained reconfigurable array (CGRA).  As our understanding of the
  application grows, the CGRA design evolves, and we have developed a suite of
  tools that tune application code, the compiler, and the CGRA to increase the
  efficiency of the resulting implementation. To meet our continued need to
  update parts of the system while maintaining the end-to-end flow, we have
  created DSL-based hardware generators that not only provide the Verilog
  needed for the implementation of the CGRA, but also create the collateral
  that the compiler/mapper/place and route system needs to configure its
  operation. This work provides a systematic approach for desiging and evolving
  high-performance and energy-efficient hardware-software systems for any
  application domain.
  </p>"
  venue: "Design Automation Conference"
  venuenote: "DAC"
  year: 2020
  month: July
  authors:
    - bahr
    - barrett
    - bhagdikar
    - carsello
    - daly
    - donovick
    - durst
    - fatahalian
    - feng
    - hanrahan
    - hofstee
    - horowitz
    - huff
    - kjolstad
    - kong
    - liu
    - mann
    - melchert
    - nayak
    - niemetz
    - nyengele
    - raina
    - richardson
    - setaluri
    - setter
    - sreedhar
    - strange
    - thomas
    - torng
    - truong
    - tsiskaridze
    - zhang
  pdf: /publications/aha.pdf

spaa20:
  title: "Sparse Tensor Transpositions"
  abstract: "<p>
  We present a new algorithm for transposing sparse tensors called Quesadilla.
  The algorithm converts the sparse tensor data structure to a list of
  coordinates and sorts it with a fast multi-pass radix algorithm that exploits
  knowledge of the requested transposition and the tensors input partial
  coordinate ordering to provably minimize the number of parallel partial
  sorting passes. We evaluate both a serial and a parallel implementation of
  Quesadilla on a set of 19 tensors from the FROSTT collection, a set of
  tensors taken from scientific and data analytic applications. We compare
  Quesadilla and a generalization, Top-2-sadilla to several state of the art
  approaches, including the tensor transposition routine used in the SPLATT
  tensor factorization library. In serial tests, Quesadilla was the best
  strategy for 60% of all tensor and transposition combinations and improved
  over SPLATT by at least 19% in half of the combinations. In parallel tests,
  at least one of Quesadilla or Top-2-sadilla was the best strategy for 52% of
  all tensor and transposition combinations.
  </p>"
  venue: "ACM Symposium on Parallelism in Algorithms and Architectures"
  venuenote: "SPAA brief announcement"
  year: 2020
  month: July
  authors:
    - mueller
    - ahrens
    - chou
    - kjolstad
    - amarasinghe
  tag: transposition
  pdf: /publications/taco-transposition.pdf


pldi20:
  title: "Automatic Generation of Efficient Sparse Tensor Format Conversion Routines"
  abstract: "<p>
  This paper shows how to generate code that efficiently converts sparse
  tensors between disparate storage formats (data layouts) like CSR, DIA, ELL,
  and many others. We decompose sparse tensor conversion into three logical
  phases: coordinate remapping, analysis, and assembly.  We then develop a
  language that precisely describes how different formats group together and
  order a tensor's nonzeros in memory. This enables a compiler to emit code
  that performs complex reorderings (remappings) of nonzeros when converting
  between formats.  We additionally develop a query language that can extract
  complex statistics about sparse tensors, and we show how to emit efficient
  analysis code that computes such queries. Finally, we define an abstract
  interface that captures how data structures for storing a tensor can be
  efficiently assembled given specific statistics about the tensor.  Disparate
  formats can implement this common interface, thus letting a compiler emit
  optimized sparse tensor conversion code for arbitrary combinations of a wide
  range of formats without hard-coding for any specific one.
  </p><p>
  Our evaluation shows that our technique generates sparse tensor conversion
  routines with performance between 0.99 and 2.2x that of hand-optimized
  implementations in two widely used sparse linear algebra libraries, SPARSKIT
  and Intel MKL.  By emitting code that avoids materializing temporaries, our
  technique also outperforms both libraries by between 1.4 and 3.4x for CSC/COO
  to DIA/ELL conversion.
  </p>"
  venue: "ACM SIGPLAN Conference on Programming Language Design and Implementation"
  venuenote: "PLDI"
  year: 2020
  month: June
  authors:
    - chou
    - kjolstad
    - amarasinghe
  tag: conversion
  pdf: /publications/taco-conversion.pdf
  movie: https://youtu.be/s6JRY8qcjvA
  embeddedmovie: <iframe width="560" height="315" src="https://www.youtube.com/embed/s6JRY8qcjvA" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

cgo19:
  title: "Tensor Algebra Compilation with Workspaces"
  abstract: "<p>
  This paper shows how to extend sparse tensor algebra compilers to
  introduce temporary tensors called workspaces to avoid inefficient sparse
  data structures accesses.  We develop an intermediate representation (IR) for
  tensor operations called concrete index notation that specifies when
  sub-computations occur and where they are stored.  We then describe the
  workspace transformation in this IR, how to programmatically invoke it, and
  how the IR is compiled to sparse code.  Finally, we show how the
  transformation can be used to optimize sparse tensor kernels, including
  sparse matrix multiplication, sparse tensor addition, and the matricized
  tensor times Khatri-Rao product (MTTKRP).
  </p><p>
  Our results show that the workspace transformation brings the performance of
  these kernels on par with hand-optimized implementations. For example, we
  improve the performance of MTTKRP with dense output by up to 35%, and enable
  generating sparse matrix multiplication and MTTKRP with sparse output,
  neither of which were supported by prior tensor algebra compilers.
  </p>"
  venue: "International Symposium on Code Generation and Optimization"
  venuenote: "CGO"
  year: 2019
  month: February
  authors:
    - kjolstad
    - ahrens
    - kamil
    - amarasinghe
  tag: workspaces
  pdf: /publications/taco-workspaces.pdf

oopsla18:
  title: "Format Abstraction for Sparse Tensor Algebra Compilers"
  abstract: "This paper shows how to build a sparse tensor algebra
  compiler that is agnostic to tensor formats (data layouts). We
  develop an interface that describes formats in terms of their
  capabilities and properties, and show how to build a modular code
  generator where new formats can be added as plugins. We then
  describe six implementations of the interface that compose to form
  the dense, CSR/CSF, COO, DIA, ELL, and HASH tensor formats and
  countless variants thereof. With these implementations at hand, our
  code generator can generate code to compute any tensor algebra
  expression on any combination of the aforementioned formats.  To
  demonstrate our technique, we have implemented it in the taco tensor
  algebra compiler. Our modular code generator design makes it simple
  to add support for new tensor formats, and the performance of the
  generated code is competitive with hand-optimized implementations.
  Furthermore, by extending taco to support a wider range of formats
  specialized for different application and data characteristics, we
  can improve end-user application performance. For example, if input
  data is provided in the COO format, our technique allows computing a
  single matrix-vector multiplication directly with the data in COO,
  which is up to 3.6x faster than by first converting the data to
  CSR."
  venue: "Proceedings of the ACM on Programming Languages"
  volume: 2
  issue: OOPSLA
  year: 2018
  month: October
  authors:
    - chou
    - kjolstad
    - amarasinghe
  tag: formats
  pdf: /publications/taco-formats.pdf
  movie: https://youtu.be/sQOq3Ci4tB0
  embeddedmovie: <iframe width="560" height="315" src="https://www.youtube.com/embed/sQOq3Ci4tB0" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

avancees18:
  title: "Taco: compilation et génération de code d’expressions tensorielles"
  abstract: "L’évaluation des performances du code généré démontre l’efficacité
  de cette approche tant sur des expressions simples comme un SpMV de forme bi=
  Aij× xj (figure 3a) que sur des expressions complexes impliquant des tenseurs
  d’ordre élevé comme un MTTKRP de forme Aij= Bikl× Ckj× Dlj. Elle permet
  surtout d’éliminer le compromis entre performance et exhaustivité des
  librairies existantes. La figure 3b montre pour l’expression Aij= Bij+ Cij+
  Dij les gains de performance obtenus avec taco contre les librairies qui
  implémentent cette expression. Le parallélisme pour des architectures à
  mémoire partagée est assuré par l’ajout de directives d’interfaces de
  programmation parallèle (telles que OpenMP ou Cilk) au niveau des boucles
  externes. Les travaux de recherche en cours portent sur la prise en compte de
  nouveaux formats de stockage et d’un niveau de parallélisme distribué. La
  suite logicielle taco est composée de trois couches logicielles successives
  et utilisables indépendamment: une librairie C++, un outil en ligne de
  commande et une interface web. L’outil en ligne de commande permet de
  comparer les performances des noyaux de code générés, d’étudier l’impact du
  choix des formats et d’optimiser interactivement le code généré 4. Le
  compilateur taco propose une approche générative dédiée à l’algèbre
  tensorielle et pour une infinité de formats de stockage. En s’ appuyant sur
  ce compilateur, on peut construire des langages, des bibliothèques et des
  logiciels de simulation manipulant des tenseurs denses et creux sans écrire
  manuellement le code ni sacrifier les performances."
  authors:
    - lugato
    - kjolstad
    - chou
    - amarasinghe
    - kamil
  venue: "AVANCÉES"
  volume: 12
  year: 2018
  tag: avancee
  pdf: https://scholar.google.com/scholar?oi=bibs&cluster=8503015401540923787&btnI=1&hl=en

ase17:
  title: "taco: A Tool to Generate Tensor Algebra Kernels"
  abstract: "Tensor algebra is an important computational abstraction
  that is increasingly used in data analytics, machine learning,
  engineering, and the physical sciences. However, the number of
  tensor expressions is unbounded, which makes it hard to develop and
  optimize libraries. Furthermore, the tensors are often sparse (most
  components are zero), which means the code has to traverse
  compressed formats. To support programmers we have developed taco, a
  code generation tool that generates dense, sparse, and mixed kernels
  from tensor algebra expressions. This paper describes the taco web
  and command-line tools and discusses the benefits of a code
  generator over a traditional library. See also the demo video at
  tensor-compiler.org/ase2017."
  venue: "32th IEEE/ACM International Conference on Automated Software Engineering"
  venuenote: "ASE tools paper"
  year: "2017"
  month: "November"
  authors:
    - kjolstad
    - chou
    - lugato
    - kamil
    - amarasinghe
  tag: tools
  pdf: /publications/taco-tools.pdf
  movie: https://youtu.be/eE38PC2ctFs
  embeddedmovie: <iframe width="560" height="315" src="https://www.youtube.com/embed/eE38PC2ctFs" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe> 

oopsla17:
  title: "The Tensor Algebra Compiler" 
  abstract: "Tensor algebra is a powerful tool with applications in
  machine learning, data analytics, engineering and the physical
  sciences. Tensors are often sparse and compound operations must
  frequently be computed in a single kernel for performance and to
  save memory. Programmers are left to write kernels for every
  operation of interest, with different mixes of dense and sparse
  tensors in different formats. The combinations are infinite, which
  makes it impossible to manually implement and optimize them all.
  This paper introduces the first compiler technique to automatically
  generate kernels for any compound tensor algebra operation on dense
  and sparse tensors. The technique is implemented in a C++ library
  called taco. Its performance is competitive with best-in-class
  hand-optimized kernels in popular libraries, while supporting far
  more tensor operations." 
  venue: "Proceedings of the ACM on Programming Languages"
  volume: 1
  issue: OOPSLA
  year: 2017
  month: October
  award: "Distinguished Paper Award"
  authors:
    - kjolstad 
    - kamil
    - chou
    - lugato
    - amarasinghe
  tag: taco
  pdf: /publications/taco.pdf
  movie: https://youtu.be/Kffbzf9etLE 
  embeddedmovie: <iframe width="560" height="315" src="https://www.youtube.com/embed/Kffbzf9etLE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>

tog16a:
  title: "Simit: A Language for Physical Simulation"
  abstract: "<p>
  With existing programming tools, writing high-performance simulation code is
  labor intensive and requires sacrificing readability and portability. The
  alternative is to prototype simulations in a high-level language like Matlab,
  thereby sacrificing performance. The Matlab programming model naturally
  describes the behavior of an entire physical system using the language of
  linear algebra. However, simulations also manipulate individual geometric
  elements, which are best represented using linked data structures like
  meshes. Translating between the linked data structures and linear algebra
  comes at significant cost, both to the programmer and to the machine.
  High-performance implementations avoid the cost by rephrasing the computation
  in terms of linked or index data structures, leaving the code complicated and
  monolithic, often increasing its size by an order of magnitude.
  </p><p>
  In this article, we present Simit, a new language for physical simulations
  that lets the programmer view the system both as a linked data structure in
  the form of a hypergraph and as a set of global vectors, matrices, and
  tensors depending on what is convenient at any given time. Simit provides a
  novel assembly construct that makes it conceptually easy and computationally
  efficient to move between the two abstractions. Using the information pro-
  vided by the assembly construct, the compiler generates efficient in-place
  computation on the graph. We demonstrate that Simit is easy to use: a Simit
  program is typically shorter than a Matlab program; that it is high
  performance: a Simit program running sequentially on a CPU performs compara-
  bly to hand-optimized simulations; and that it is portable: Simit programs
  can be compiled for GPUs with no change to the program, delivering 4 to 20x
  speedups over our optimized CPU code.
  </p>"
  year: "2016"
  month: "May"
  venue: "ACM Transactions on Graphics"
  venuenote: "TOG presented at SIGGRAPH 2016"
  authors:
    - kjolstad
    - kamil
    - ragan-kelley
    - levin
    - sueda
    - chen
    - vouga
    - kaufman
    - kanwar
    - matusik
    - amarasinghe
  tag: simit
  pdf: /publications/simit.pdf
  movie: https://youtu.be/raPkxhHy5ro
  image: http://groups.csail.mit.edu/commit/papers/2016/simit.jpg
  embeddedmovie: <iframe width="560" height="315" src="https://www.youtube.com/embed/raPkxhHy5ro" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>

tog16b:
  title: "Why New Programming Languages for Simulation?"
  abstract: "<p>
  Simulations are complicated, performance-critical applications that combine
  sophisticated computer science data structures with advanced mathematical
  computation. Best practice suggests using optimized linear algebra libraries,
  yet programmers of high- performance simulations invariably abandon this
  ideal in order to optimize computation around application data structures.
  Getting good performance requires a lot of human effort to manage data
  layout, vectorize, and parallelize code. And the situation is getting worse.
  High-performance systems in the near future will need to support concurrent
  execution on GPUs, multicore CPUs, a host of new architectures, and across
  distributed systems.
  </p><p>
  In response to similar trends, graphics researchers have proposed new
  programming languages to manage and abstract away from hardware complexity.
  Renderman [Upstill 1989], GPU Shading Languages, Cuda [Nvidia 2008], and more
  recently Halide [Ragan-Kelley et al. 2013] and Darkroom [Hegarty et al. 2014]
  are excellent examples. These languages promise considerable reductions in
  programmer effort, as reflected both by the amount of code that must be
  written and the degree to which programmers must optimize for specific
  hardware.  We believe simulation is now primed to move to such programming
  languages. Although simulation is a complicated domain, we are starting to
  understand how to represent a large class of simulations with simple,
  general, and flexible high level programming language concepts built on solid
  foundations. We believe the ideas behind the relational algebra and modern
  databases have direct relevance to simulation, and we expect to see similar
  benefits to those the database community has accrued.
  </p><p>
  In these proceedings, two such languages are presented: Ebb [Bernstein et al.
  2016] and Simit [Kjolstad et al. 2016]. These languages were developed
  independently by two separate groups centered at Stanford and MIT, and we
  believe they are first steps in a larger roadmap. We will let the articles
  speak for themselves on the strengths of their different operators and data
  structures. However, we encourage readers to note the common foundations we
  will lay out and join us in exploring a fascinating new research direction.
  </p>"
  venue: "ACM Transactions on Graphics"
  venuenote: "TOG perspective"
  year: 2016
  month: May
  authors:
    - bernstein
    - kjolstad
  tag: why
  pdf: /publications/why_simulation_languages.pdf

eurompi13:
  title: "MPI Datatype Processing using Runtime Compilation"
  abstract: "Data packing before and after communication can make up as much as
  90% of the communication time on modern computers. Despite MPI's well-defined
  datatype interface for non-contiguous data access, many codes use manual pack
  loops for performance reasons. Programmers write access-pattern specific pack
  loops (e.g., do manual unrolling) for which compilers emit optimized code. In
  contrast, MPI implementations in use today interpret datatypes at pack time,
  resulting in high overheads. In this work we explore the effectiveness of
  using runtime compilation techniques to generate efficient and optimized pack
  code for MPI datatypes at commit time. Thus, none of the overhead of datatype
  interpretation is incurred at pack time and pack setup is as fast as calling
  a function pointer. We have implemented a library called libpack that can be
  used to compile and (un)pack MPI datatypes. The library optimizes the
  datatype representation and uses the LLVM framework to produce vectorized
  machine code for each datatype at commit time. We show several examples of
  how MPI datatype pack functions benefit from runtime compilation and analyze
  the performance of compiled pack functions for the data access patterns in
  many applications. We show that the pack/unpack functions generated by our
  packing library are seven times faster than those of prevalent MPI
  implementations for 73% of the datatypes used in MILC and in many cases
  outperform manual pack loops."
  venue: "20th European MPI Users' Group Meeting"
  year: "2013"
  month: "September"
  award: "Best Paper Award" # (1/25)
  authors:
    - schneider
    - kjolstad
    - hoefler
  tag: datatypecompilation
  pdf: /publications/datatype_compilation.pdf

ppopp12:
  title: "Automatic Datatype Generation and Optimization"
  abstract: "Many high performance applications spend considerable time packing
  noncontiguous data into contiguous communication buffers. MPI Datatypes
  provide an alternative by describing noncontiguous data layouts. This allows
  sophisticated hardware to retrieve data directly from application data
  structures. However, packing codes in real-world applications are often
  complex and specifying equivalent datatypes is difficult, time-consuming, and
  error prone. We present an algorithm that automates the transformation. We
  have implemented the algorithm in a tool that transforms packing code to MPI
  Datatypes, and evaluated it by transforming 90 packing codes from the NAS
  Parallel Benchmarks. The transformation allows easy porting of applications
  to new machines that benefit from datatypes, thus improving programmer
  productivity."
  venue: "17th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming"
  venuenote: "PPoPP short paper"
  year: "2012"
  month: "February"
  authors:
    - kjolstad
    - hoefler
    - snir
  tag: datatypegeneration
  pdf: /publications/datatype_generation.pdf

icse11:
  title: "Transformation for Class Immutability"
  abstract: "It is common for object-oriented programs to have both mutable and
  immutable classes. Immutable classes simplify programing because the
  programmer does not have to reason about side-effects. Sometimes programmers
  write immutable classes from scratch, other times they transform mutable into
  immutable classes. To transform a mutable class, programmers must find all
  methods that mutate its transitive state and all objects that can enter or
  escape the state of the class. The analyses are non-trivial and the rewriting
  is tedious. Fortunately, this can be automated.
  <br/>
  We present an algorithm and a tool, Immutator, that enables the programmer to
  safely transform a mutable class into an immutable class. Two case studies
  and one controlled experiment show that Immutator is useful. It (i) reduces
  the burden of making classes immutable, (ii) is fast enough to be used
  interactively, and (iii) is much safer than manual transformations."
  venue: "33rd International Conference on Software Engineering"
  venuenote: "ICSE"
  year: "2011"
  month: "May"
  authors:
    - kjolstad
    - dig
    - acevedo
    - snir
  tag: immutator
  pdf: /publications/immutator.pdf

cap10:
  title: "Bringing the HPC Programmer's IDE into the 21st Century through Refactoring"
  abstract: "Programming tools for High Performance Computing are lagging
  behind the tools that have improved the productivity of desktop programmers.
  The increasing complexity of HPC codes, the growing number of cores that they
  must utilize, their long life-span, and the plethora of desirable source code
  optimizations and hardware platforms make HPC codes hard to maintain.
  Refactoring tools can enable HPC programmers to explore the space of
  performance optimizations and parallel constructs safely and efficiently.
  <br/>
  This position paper presents our view on how HPC programming tools should
  evolve, a growing catalog of refactorings for HPC programmers, and reports on
  our initial effort to automate some of these refactorings."
  venue: "SPLASH 2010 Workshop on Concurrency for the Application Programmer"
  year: 2010
  month: "October"
  authors:
    - kjolstad
    - dig
    - snir
  tag: hpcrefactoring
  pdf: /publications/hpc_refactoring.pdf

paraplop10:
  title: "Ghost Cell Pattern"
  abstract: "Many problems consist of a structured grid of points that are
  updated repeatedly based on the values of a fixed set of neighboring points
  in the same grid. To parallelize these problems we can geometrically divide
  the grid into chunks that are processed by different processors. One
  challenge with this approach is that the update of points at the periphery of
  a chunk requires values from neighboring chunks. These are often located in
  remote memory belonging to different processes. The naive implementation
  results in a lot of time spent on communication leaving less time for useful
  computation. By using the Ghost Cell Pattern communication overhead can be
  reduced. This results in faster time to completion."
  venue: "2nd Annual Workshop on Parallel Programming Patterns"
  year: "2010"
  month: "March"
  authors:
    - kjolstad
    - snir
  tag: ghost
  pdf: /publications/ghost_cell_pattern.pdf
